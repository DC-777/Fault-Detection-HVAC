{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit, KFold, RepeatedKFold, \\\n",
    "                                    train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from sklearn.metrics import precision_score, mean_squared_error, r2_score, make_scorer, adjusted_rand_score, \\\n",
    "                    accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score, recall_score\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "import scipy.stats as st\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import pprint as pp\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Chiller_Data = pd.read_excel('Refrigerant_Leak_Fault_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Chiller_Data = Chiller_Data.loc[Chiller_Data['kW'] != 1.682000e-45]\n",
    "Chiller_Data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Chiller_Data['Target_EPS'] = (Chiller_Data['TRC_sub'])/(Chiller_Data['TRC']-Chiller_Data['TCI'])\n",
    "Chiller_Data['Target_LMTD'] = (Chiller_Data['TCO']-Chiller_Data['TCI'])/np.log((Chiller_Data['TRC']-Chiller_Data['TCI'])/(Chiller_Data['TRC']-Chiller_Data['TCO']))\n",
    "Chiller_Data['Lag1'] = (Chiller_Data['Target_EPS'].shift(1))\n",
    "Chiller_Data['Lag2'] = (Chiller_Data['Target_LMTD'].shift(1))\n",
    "Chiller_Data.dropna(axis=0,inplace=True)\n",
    "#Time_data = Chiller_Data['Time (minutes)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = Chiller_Data[['Target_EPS','Target_LMTD']].as_matrix()\n",
    "True_Labels = Chiller_Data['Label'].as_matrix()\n",
    "Chiller_Data.drop(['Target_EPS','Target_LMTD','Label','Time (minutes)'], axis=1, inplace=True)\n",
    "X = Chiller_Data.as_matrix()\n",
    "Feature_Names = list(Chiller_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.55, shuffle=False)\n",
    "TL_train, TL_Test = train_test_split(True_Labels, test_size=0.55, shuffle=False)\n",
    "#DT_train, DT_Test = train_test_split(Time_data, test_size=0.55, shuffle=False)\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dyn_threshold(A_EPS, P_EPS, A_LMTD, P_LMTD, I, N):\n",
    "    # Control false alarm rates by tuning I and N. eg. increase I or N to reduce false alarms\n",
    "    threshold_EPS = np.zeros(I-1)\n",
    "    threshold_EPS[0:(I-1)] = P_EPS[0:(I-1)]\n",
    "    threshold_LMTD = np.zeros(I-1)\n",
    "    threshold_LMTD[0:(I-1)] = P_LMTD[0:(I-1)]\n",
    "    labels = np.zeros(I-1)\n",
    "    for k in np.arange(I,len(P_EPS)+1):\n",
    "        mu_EPS = np.mean(P_EPS[(k-I):k])\n",
    "        sigma_EPS = np.std(P_EPS[(k-I):k])\n",
    "        T_EPS = mu_EPS - N*sigma_EPS\n",
    "        threshold_EPS = np.append(threshold_EPS,T_EPS)\n",
    "        mu_LMTD = np.mean(P_LMTD[(k-I):k])\n",
    "        sigma_LMTD = np.std(P_LMTD[(k-I):k])\n",
    "        T_LMTD = mu_LMTD - N*sigma_LMTD\n",
    "        threshold_LMTD = np.append(threshold_LMTD,T_LMTD)\n",
    "        \n",
    "        if (A_EPS[k-1] < threshold_EPS[k-1] or A_LMTD[k-1] < threshold_LMTD[k-1]) :\n",
    "            labels = np.append(labels,1)\n",
    "        else:\n",
    "            labels = np.append(labels,0)\n",
    "    return labels, threshold_EPS, threshold_LMTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "np.random.seed(7)\n",
    "########################################################################################\n",
    "# Regression\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=7)\n",
    "scoring_param = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "\n",
    "rfecv = RFECV(estimator=RandomForestRegressor(n_jobs=-1), step=1, cv=kf, scoring=scoring_param, n_jobs=-1)\n",
    "FS_model = rfecv.fit(X_train, y_train[:,0])\n",
    "\n",
    "ranks = FS_model.ranking_\n",
    "FN =[]\n",
    "for i in range(len(ranks)):\n",
    "    if ranks[i] == 1:\n",
    "        FN.append(Feature_Names[i])\n",
    "print(FN)\n",
    "\n",
    "X = Chiller_Data[FN].as_matrix()\n",
    "X_train, X_test = train_test_split(X, test_size=0.55, shuffle=False)\n",
    "\n",
    "NE = [int(i) for i in np.linspace(100,1000,num=10)]\n",
    "p_grid = dict()\n",
    "p_grid = dict(n_estimators = NE)\n",
    "\n",
    "model = GridSearchCV(estimator = RandomForestRegressor(n_jobs=-1), param_grid = p_grid, scoring = scoring_param, cv = kf, \n",
    "                     n_jobs=-1)\n",
    "model.fit(X_train, y_train[:,0])\n",
    "    \n",
    "params = model.best_params_\n",
    "print(\"Best Est: %s\" % (params['n_estimators']))\n",
    "    \n",
    "P_EPS = model.predict(X_test)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "rfecv = RFECV(estimator=RandomForestRegressor(n_jobs=-1), step=1, cv=kf, scoring=scoring_param, n_jobs=-1)\n",
    "FS_model = rfecv.fit(X_train, y_train[:,1])\n",
    "\n",
    "ranks = FS_model.ranking_\n",
    "FN =[]\n",
    "for i in range(len(ranks)):\n",
    "    if ranks[i] == 1:\n",
    "        FN.append(Feature_Names[i])\n",
    "print(FN)\n",
    "\n",
    "X = Chiller_Data[FN].as_matrix()\n",
    "X_train, X_test = train_test_split(X, test_size=0.55, shuffle=False)\n",
    "\n",
    "NE = [int(i) for i in np.linspace(100,1000,num=10)]\n",
    "p_grid = dict()\n",
    "p_grid = dict(n_estimators = NE)\n",
    "\n",
    "model = GridSearchCV(estimator = RandomForestRegressor(n_jobs=-1), param_grid = p_grid, scoring = scoring_param, cv = kf, \n",
    "                     n_jobs=-1)\n",
    "model.fit(X_train, y_train[:,1])\n",
    "    \n",
    "params = model.best_params_\n",
    "print(\"Best Est: %s\" % (params['n_estimators']))\n",
    "    \n",
    "P_LMTD = model.predict(X_test)\n",
    "\n",
    "\n",
    "Labels, Threshold_EPS, Threshold_LMTD = calc_dyn_threshold(y_test[:,0], P_EPS, y_test[:,1], P_LMTD, 2, 2)\n",
    "Temp = pd.DataFrame(data={'Labels':TL_Test, 'Pred_Labels': Labels})\n",
    "\n",
    "print(\"########################################################################################\")\n",
    "print(\"Confusion Matrix - testing:\")\n",
    "print(confusion_matrix(Temp['Labels'], Temp['Pred_Labels']))\n",
    "tn, fp, fn, tp = confusion_matrix(Temp['Labels'], Temp['Pred_Labels']).ravel()\n",
    "print(\"True Negative, False Positive, False Negative, True Positive {}.\".format([tn, fp, fn, tp]))\n",
    "print(\"False positive means false alarms\")\n",
    "print(\"False Negative means missed faults\")\n",
    "print(\"########################################################################################\")\n",
    "print(\"Classification Report - testing:\")\n",
    "print(classification_report(Temp['Labels'], Temp['Pred_Labels'], target_names=['Normal', 'Fault']))\n",
    "print(\"########################################################################################\")\n",
    "print(\"Accuracy - testing: %0.3f\" % accuracy_score(Temp['Labels'], Temp['Pred_Labels']))\n",
    "print(\"########################################################################################\")\n",
    "print(\"ROC AUC score - testing: %0.3f\" % roc_auc_score(Temp['Labels'], Temp['Pred_Labels']))\n",
    "print(\"########################################################################################\")\n",
    "########################################################################################\n",
    "    \n",
    "t1 = time()\n",
    "print('Time taken for this trial %f' %(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
