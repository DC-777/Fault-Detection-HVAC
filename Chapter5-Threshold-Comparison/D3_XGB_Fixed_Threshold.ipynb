{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit, KFold, RepeatedKFold, \\\n",
    "                                    train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from sklearn.metrics import precision_score, mean_squared_error, r2_score, make_scorer, adjusted_rand_score, \\\n",
    "                    accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score, recall_score\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "import scipy.stats as st\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pprint as pp\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Climate_Data = pd.read_excel('Climate_Data.xls')\n",
    "#######################################################################################################################\n",
    "Energy_Data_mean = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).mean()\n",
    "Energy_Data_mean = Energy_Data_mean[['Day of Week', 'Is Holiday', 'Daylight Savings', 'DHI', 'DNI', 'Dew Point', \n",
    "                                     'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_mean.columns = ['Day_of_Week', 'Is_Holiday', 'Daylight_Savings', 'DHI_AVG', 'DNI_AVG', 'Dew Point_AVG', \n",
    "                            'Temperature_AVG', 'Relative Humidity_AVG']\n",
    "#######################################################################################################################\n",
    "Energy_Data_sum = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).sum()\n",
    "Energy_Data_sum = Energy_Data_sum[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_sum.columns = ['DHI_SUM', 'DNI_SUM', 'Dew Point_SUM', 'Temperature_SUM', 'Relative Humidity_SUM']\n",
    "#######################################################################################################################\n",
    "Energy_Data_max = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).max()\n",
    "Energy_Data_max = Energy_Data_max[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_max.columns = ['DHI_MAX', 'DNI_MAX', 'Dew Point_MAX', 'Temperature_MAX', 'Relative Humidity_MAX']\n",
    "#######################################################################################################################\n",
    "Energy_Data_std = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).std()\n",
    "Energy_Data_std = Energy_Data_std[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_std.columns = ['DHI_STD', 'DNI_STD', 'Dew Point_STD', 'Temperature_STD', 'Relative Humidity_STD']\n",
    "#######################################################################################################################\n",
    "Energy_Data_min = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).min()\n",
    "Energy_Data_min = Energy_Data_min[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_min.columns = ['DHI_MIN', 'DNI_MIN', 'Dew Point_MIN', 'Temperature_MIN', 'Relative Humidity_MIN']\n",
    "#######################################################################################################################\n",
    "Energy_Data = pd.concat([Energy_Data_mean, Energy_Data_sum, Energy_Data_max, Energy_Data_std, Energy_Data_min], axis=1)\n",
    "Energy_Data.reset_index(inplace=True)\n",
    "Energy_Data[['Energy_Consumption', 'True_Labels']] = pd.read_excel('EnergyData_D3.xlsx')\n",
    "#######################################################################################################################\n",
    "Energy_Data['Lag1'] = (Energy_Data['Energy_Consumption'].shift(1))\n",
    "Energy_Data.dropna(axis=0,inplace=True)\n",
    "#######################################################################################################################\n",
    "Energy_Data['Date_Time'] = pd.to_datetime(pd.DataFrame({'year': Energy_Data['Year'],'month': Energy_Data['Month'] + 1,\n",
    "                                                        'day': Energy_Data['Day of Month']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature_Names = ['Month','Day_of_Week', 'Is_Holiday', 'Daylight_Savings', 'DHI_AVG', 'DNI_AVG', 'Dew Point_AVG', \n",
    "                 'Temperature_AVG', 'Relative Humidity_AVG', 'DHI_SUM', 'DNI_SUM', 'Dew Point_SUM', 'Temperature_SUM', \n",
    "                 'Relative Humidity_SUM', 'DHI_MAX', 'DNI_MAX', 'Dew Point_MAX', 'Temperature_MAX', \n",
    "                 'Relative Humidity_MAX', 'DHI_STD', 'DNI_STD', 'Dew Point_STD', 'Temperature_STD', \n",
    "                 'Relative Humidity_STD', 'DHI_MIN', 'DNI_MIN', 'Dew Point_MIN', 'Temperature_MIN', \n",
    "                 'Relative Humidity_MIN', 'Lag1']\n",
    "\n",
    "X = Energy_Data[Feature_Names].as_matrix()\n",
    "y = Energy_Data['Energy_Consumption'].as_matrix()\n",
    "True_Labels = Energy_Data['True_Labels'].as_matrix()\n",
    "date_time = Energy_Data['Date_Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "# To test anomaly detector\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "TL_train, TL_Test = train_test_split(True_Labels, test_size=0.5, shuffle=False)\n",
    "DT_train, DT_Test = train_test_split(date_time, test_size=0.5, shuffle=False)\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Range: %0.3f\" % (np.max(y_train) - np.min(y_train)))\n",
    "print(\"Mean: %0.3f\" % (np.mean(y_train)))\n",
    "print(\"Median: %0.3f\" % (np.median(y_train)))\n",
    "print(\"Std. Deviation: %0.3f\" % (np.std(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def anomalyDetector():\n",
    "    t0 = time()\n",
    "    np.random.seed(7)\n",
    "    ########################################################################################\n",
    "    # Regression\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    scoring_param = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "    \n",
    "    rfecv = RFECV(estimator=XGBRegressor(n_jobs=-1), step=1, cv=kf, scoring=scoring_param)\n",
    "    FS_model = rfecv.fit(X_train, y_train)\n",
    "    \n",
    "    ranks = FS_model.ranking_\n",
    "    FN =[]\n",
    "    for i in range(len(ranks)):\n",
    "        if ranks[i] == 1:\n",
    "            FN.append(Feature_Names[i])    \n",
    "    print(FN)\n",
    "    \n",
    "    X = Energy_Data[FN].as_matrix()\n",
    "    X_train_transformed, X_test_transformed = train_test_split(X, test_size=0.5, shuffle=False)\n",
    "    \n",
    "    MD = [int(i) for i in np.linspace(1,20,num=5)]\n",
    "    LR = np.linspace(0.001,0.1,num=5)\n",
    "    NE = [int(i) for i in np.linspace(100,1000,num=10)]\n",
    "    p_grid = dict()\n",
    "    p_grid = dict(max_depth =  MD,\n",
    "                  learning_rate = LR,\n",
    "                  n_estimators = NE)\n",
    "    \n",
    "    model = GridSearchCV(estimator = XGBRegressor(n_jobs=-1), param_grid = p_grid, scoring = scoring_param, cv = kf)\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    params = model.best_params_\n",
    "    print(\"Best max depth: %s Best LR: %f Best Est: %s\" % (params['max_depth'], params['learning_rate'], params['n_estimators']))\n",
    "    \n",
    "    YTest_Pred = model.predict(X_train_transformed)\n",
    "    Y_Test_Pred = model.predict(X_test_transformed)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test,Y_Test_Pred))\n",
    "    data_range = y_test.max() - y_test.min()\n",
    "    NRMSE = (rmse/data_range) * 100.0\n",
    "    RSQ = r2_score(y_test,Y_Test_Pred)\n",
    "    print(\"Normalized RMSE: %0.3f\" % NRMSE)\n",
    "    print(\"R-squared: %0.3f\" % RSQ)\n",
    "    ########################################################################################\n",
    "    ########################## Set fixed threshold ################################################\n",
    "    ############################################################################################\n",
    "    Temp = pd.DataFrame(data={'Date_Time': DT_Test, 'Actual': y_test, 'Predicted':Y_Test_Pred, 'Labels':TL_Test})\n",
    "    Temp.sort_values(by=['Date_Time'],inplace=True)\n",
    "    Temp['Threshold'] = np.zeros(len(Temp.index))\n",
    "    Temp['Threshold'] = np.mean(YTest_Pred) + 2*np.std(YTest_Pred)\n",
    "    Labels = np.zeros(0)\n",
    "    for i in np.arange(0,len(Temp)):\n",
    "        if Temp['Actual'].values[i] > Temp['Threshold'].values[i]:\n",
    "            Labels = np.append(Labels,1)\n",
    "        else:\n",
    "            Labels = np.append(Labels,0)\n",
    "    Temp['Pred_Labels'] = Labels\n",
    "    Temp = Temp[Temp['Date_Time'].between('2016-01-01','2016-12-31')]\n",
    "    print(\"########################################################################################\")\n",
    "    print(\"Confusion Matrix - testing:\")\n",
    "    print(confusion_matrix(Temp['Labels'], Temp['Pred_Labels']))\n",
    "    tn, fp, fn, tp = confusion_matrix(Temp['Labels'], Temp['Pred_Labels']).ravel()\n",
    "    print(\"True Negative, False Positive, False Negative, True Positive {}.\".format([tn, fp, fn, tp]))\n",
    "    print(\"False positive means false alarms\")\n",
    "    print(\"False Negative means missed faults\")\n",
    "    print(\"########################################################################################\")\n",
    "    print(\"Classification Report - testing:\")\n",
    "    print(classification_report(Temp['Labels'], Temp['Pred_Labels'], target_names=['class 0', 'class 1']))\n",
    "    print(\"########################################################################################\")\n",
    "    print(\"Accuracy - testing: %0.3f\" % accuracy_score(Temp['Labels'], Temp['Pred_Labels']))\n",
    "    print(\"########################################################################################\")\n",
    "    print(\"ROC AUC score - testing: %0.3f\" % roc_auc_score(Temp['Labels'], Temp['Pred_Labels']))\n",
    "    print(\"########################################################################################\")\n",
    "    ############################################################################################\n",
    "    \n",
    "    fig = plt.figure(figsize=(25,20))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    Data_0 = Temp.loc[Temp['Pred_Labels'][Temp['Pred_Labels']==0].index]\n",
    "    Data_1 = Temp.loc[Temp['Pred_Labels'][Temp['Pred_Labels']==1].index]\n",
    "    ax.scatter(Data_0['Date_Time'].dt.to_pydatetime(), Data_0['Actual'], c=plt.cm.coolwarm(0.), s=200, \n",
    "               edgecolors='y', marker='o', label=u'Predicted normal data')\n",
    "    ax.scatter(Data_1['Date_Time'].dt.to_pydatetime(), Data_1['Actual'], c=plt.cm.coolwarm(1.), s=200, \n",
    "               edgecolors='y', marker='^', label=u'Predicted fault data')\n",
    "    plt.plot(Temp['Date_Time'].dt.to_pydatetime(), Temp['Predicted'], 'c-*', lw = 4, ms = 5, label=u'XGBoost Prediction')\n",
    "    plt.plot(Temp['Date_Time'].dt.to_pydatetime(), Temp['Threshold'], 'k--', lw = 4, label=u'Dynamic threshold')\n",
    "    plt.xlabel('Date Time',fontsize=30)\n",
    "    plt.ylabel('Energy Consumption [J]',fontsize=30)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.legend(loc='best',fontsize=30)\n",
    "    plt.savefig('M1-D3-XGB-Fixed-Threshold-Predicted-Labels')\n",
    "    \n",
    "    t1 = time()\n",
    "    print(\"Time taken: %0.3f\" % (t1-t0))\n",
    "    #########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = anomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
