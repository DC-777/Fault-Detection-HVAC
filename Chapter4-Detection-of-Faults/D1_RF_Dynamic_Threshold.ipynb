{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit, KFold, RepeatedKFold, \\\n",
    "                                    train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "from sklearn.metrics import precision_score, mean_squared_error, r2_score, make_scorer, adjusted_rand_score, \\\n",
    "                    accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score, recall_score\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "import scipy.stats as st\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pprint as pp\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Climate_Data = pd.read_excel('Climate_Data.xls')\n",
    "#######################################################################################################################\n",
    "Energy_Data_mean = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).mean()\n",
    "Energy_Data_mean = Energy_Data_mean[['Day of Week', 'Is Holiday', 'Daylight Savings', 'DHI', 'DNI', 'Dew Point', \n",
    "                                     'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_mean.columns = ['Day_of_Week', 'Is_Holiday', 'Daylight_Savings', 'DHI_AVG', 'DNI_AVG', 'Dew Point_AVG', \n",
    "                            'Temperature_AVG', 'Relative Humidity_AVG']\n",
    "#######################################################################################################################\n",
    "Energy_Data_sum = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).sum()\n",
    "Energy_Data_sum = Energy_Data_sum[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_sum.columns = ['DHI_SUM', 'DNI_SUM', 'Dew Point_SUM', 'Temperature_SUM', 'Relative Humidity_SUM']\n",
    "#######################################################################################################################\n",
    "Energy_Data_max = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).max()\n",
    "Energy_Data_max = Energy_Data_max[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_max.columns = ['DHI_MAX', 'DNI_MAX', 'Dew Point_MAX', 'Temperature_MAX', 'Relative Humidity_MAX']\n",
    "#######################################################################################################################\n",
    "Energy_Data_std = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).std()\n",
    "Energy_Data_std = Energy_Data_std[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_std.columns = ['DHI_STD', 'DNI_STD', 'Dew Point_STD', 'Temperature_STD', 'Relative Humidity_STD']\n",
    "#######################################################################################################################\n",
    "Energy_Data_min = Climate_Data.groupby(['Year', 'Month', 'Day of Month']).min()\n",
    "Energy_Data_min = Energy_Data_min[['DHI', 'DNI', 'Dew Point', 'Temperature', 'Relative Humidity']]\n",
    "Energy_Data_min.columns = ['DHI_MIN', 'DNI_MIN', 'Dew Point_MIN', 'Temperature_MIN', 'Relative Humidity_MIN']\n",
    "#######################################################################################################################\n",
    "Energy_Data = pd.concat([Energy_Data_mean, Energy_Data_sum, Energy_Data_max, Energy_Data_std, Energy_Data_min], axis=1)\n",
    "Energy_Data.reset_index(inplace=True)\n",
    "Energy_Data[['Energy_Consumption', 'True_Labels']] = pd.read_excel('EnergyData_D1.xlsx')\n",
    "#######################################################################################################################\n",
    "Energy_Data['Lag1'] = (Energy_Data['Energy_Consumption'].shift(1))\n",
    "Energy_Data.dropna(axis=0,inplace=True)\n",
    "#######################################################################################################################\n",
    "Energy_Data['Date_Time'] = pd.to_datetime(pd.DataFrame({'year': Energy_Data['Year'],'month': Energy_Data['Month'] + 1,\n",
    "                                                        'day': Energy_Data['Day of Month']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature_Names = ['Month','Day_of_Week', 'Is_Holiday', 'Daylight_Savings', 'DHI_AVG', 'DNI_AVG', 'Dew Point_AVG', \n",
    "                 'Temperature_AVG', 'Relative Humidity_AVG', 'DHI_SUM', 'DNI_SUM', 'Dew Point_SUM', 'Temperature_SUM', \n",
    "                 'Relative Humidity_SUM', 'DHI_MAX', 'DNI_MAX', 'Dew Point_MAX', 'Temperature_MAX', \n",
    "                 'Relative Humidity_MAX', 'DHI_STD', 'DNI_STD', 'Dew Point_STD', 'Temperature_STD', \n",
    "                 'Relative Humidity_STD', 'DHI_MIN', 'DNI_MIN', 'Dew Point_MIN', 'Temperature_MIN', \n",
    "                 'Relative Humidity_MIN', 'Lag1']\n",
    "\n",
    "X = Energy_Data[Feature_Names].as_matrix()\n",
    "y = Energy_Data['Energy_Consumption'].as_matrix()\n",
    "True_Labels = Energy_Data['True_Labels'].as_matrix()\n",
    "date_time = Energy_Data['Date_Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "# To test anomaly detector\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "TL_train, TL_Test = train_test_split(True_Labels, test_size=0.5, shuffle=False)\n",
    "DT_train, DT_Test = train_test_split(date_time, test_size=0.5, shuffle=False)\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_dyn_threshold(A, P, I, N):\n",
    "    # Control false alarm rates by tuning I and N. eg. increase I or N to reduce false alarms\n",
    "    threshold = np.zeros(I-1)\n",
    "    threshold[0:(I-1)] = P[0:(I-1)]\n",
    "    labels = np.zeros(I-1)\n",
    "    for k in np.arange(I,len(P)+1):\n",
    "        #print(k)\n",
    "        mu = np.mean(P[(k-I):k])\n",
    "        #mu = np.mean(P[0:k])\n",
    "        #print(mu)\n",
    "        sigma = np.std(P[(k-I):k])\n",
    "        #sigma = np.std(P[0:k])\n",
    "        #print(sigma)\n",
    "        T = mu + N*sigma\n",
    "        #print(T)\n",
    "        threshold = np.append(threshold,T)\n",
    "        #print(threshold)\n",
    "        if (A[k-1] > threshold[k-1]) :\n",
    "            labels = np.append(labels,1)\n",
    "        else:\n",
    "            labels = np.append(labels,0)\n",
    "    #print(P, labels, threshold)\n",
    "    return labels, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def anomalyDetector():\n",
    "    t0 = time()\n",
    "    np.random.seed(7)\n",
    "    ########################################################################################\n",
    "    # Regression\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    scoring_param = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "    \n",
    "    rfecv = RFECV(estimator=RandomForestRegressor(n_jobs=-1), step=1, cv=kf, scoring=scoring_param)\n",
    "    FS_model = rfecv.fit(X_train, y_train)\n",
    "    \n",
    "    ranks = FS_model.ranking_\n",
    "    FN =[]\n",
    "    for i in range(len(ranks)):\n",
    "        if ranks[i] == 1:\n",
    "            FN.append(Feature_Names[i])    \n",
    "    print(FN)\n",
    "    \n",
    "    X = Energy_Data[FN].as_matrix()\n",
    "    X_train_transformed, X_test_transformed = train_test_split(X, test_size=0.5, shuffle=False)\n",
    "    \n",
    "    NE = [int(i) for i in np.linspace(100,1000,num=10)]\n",
    "    p_grid = dict()\n",
    "    p_grid = dict(n_estimators = NE)\n",
    "    \n",
    "    model = GridSearchCV(estimator = RandomForestRegressor(n_jobs=-1), param_grid = p_grid, \n",
    "                         scoring = scoring_param, cv = kf)\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    params = model.best_params_\n",
    "    print(\"Best Est: %s\" % (params['n_estimators']))\n",
    "    \n",
    "    Y_Test_Pred = model.predict(X_test_transformed)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test,Y_Test_Pred))\n",
    "    data_range = y_test.max() - y_test.min()\n",
    "    NRMSE = (rmse/data_range) * 100.0\n",
    "    RSQ = r2_score(y_test,Y_Test_Pred)\n",
    "    print(\"Normalized RMSE: %0.3f\" % NRMSE)\n",
    "    print(\"R-squared: %0.3f\" % RSQ)\n",
    "    ########################################################################################\n",
    "    # Calculate dynamic threshold\n",
    "    \n",
    "    #Testing\n",
    "    Labels, Threshold = calc_dyn_threshold(y_test, Y_Test_Pred, 2, 2)\n",
    "    Temp = pd.DataFrame(data={'Date_Time': DT_Test, 'Actual': y_test, 'Predicted':Y_Test_Pred, 'Labels':TL_Test, \n",
    "                               'Threshold':Threshold, 'Pred_Labels': Labels})\n",
    "    Temp.sort_values(by=['Date_Time'],inplace=True)\n",
    "    Temp = Temp[Temp['Date_Time'].between('2016-04-01','2016-11-15')]\n",
    "    \n",
    "    # Establishing the relationship between I, N, and model effectiveness\n",
    "    I = np.arange(2,31)\n",
    "    N = np.arange(1,4)\n",
    "    scores = np.zeros((len(I)*len(N), 6))\n",
    "    ind = 0\n",
    "    for i in I:\n",
    "        for n in N:\n",
    "            Labels, Threshold = calc_dyn_threshold(np.array(Temp['Actual']), np.array(Temp['Predicted']), i, n)\n",
    "            scores[ind,0] = i\n",
    "            scores[ind,1] = n\n",
    "            scores[ind,2] = roc_auc_score(np.array(Temp['Labels']), Labels)\n",
    "            scores[ind,3] = precision_score(np.array(Temp['Labels']), Labels)\n",
    "            scores[ind,4] = recall_score(np.array(Temp['Labels']), Labels)\n",
    "            scores[ind,5] = f1_score(np.array(Temp['Labels']), Labels)\n",
    "            ind = ind + 1\n",
    "    best_i = scores[scores[:,2].argmax(),0]\n",
    "    best_n = scores[scores[:,2].argmax(),1]\n",
    "    print(best_i,best_n)\n",
    "    \n",
    "    print(\"########################################################################################\")\n",
    "    print(\"Confusion Matrix - testing:\")\n",
    "    print(confusion_matrix(Temp['Labels'], Temp['Pred_Labels']))\n",
    "    tn, fp, fn, tp = confusion_matrix(Temp['Labels'], Temp['Pred_Labels']).ravel()\n",
    "    print(\"True Negative, False Positive, False Negative, True Positive {}.\".format([tn, fp, fn, tp]))\n",
    "    print(\"False positive means false alarms\")\n",
    "    print(\"False Negative means missed faults\")\n",
    "    print(\"########################################################################################\")\n",
    "    print(\"Classification Report - testing:\")\n",
    "    print(classification_report(Temp['Labels'], Temp['Pred_Labels'], target_names=['Normal', 'Fault']))\n",
    "    print(\"########################################################################################\")\n",
    "    print(\"Accuracy - testing: %0.3f\" % accuracy_score(Temp['Labels'], Temp['Pred_Labels']))\n",
    "    print(\"########################################################################################\")\n",
    "    print(\"ROC AUC score - testing: %0.3f\" % roc_auc_score(Temp['Labels'], Temp['Pred_Labels']))\n",
    "    print(\"########################################################################################\")\n",
    "    ########################################################################################\n",
    "    \n",
    "    fig = plt.figure(figsize=(25,20))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    Data_0 = Temp.loc[Temp['Labels'][Temp['Labels']==0].index]\n",
    "    Data_1 = Temp.loc[Temp['Labels'][Temp['Labels']==1].index]\n",
    "    ax.scatter(Data_0['Date_Time'].dt.to_pydatetime(), Data_0['Actual'], c=plt.cm.coolwarm(0.), s=200, \n",
    "               edgecolors='y', marker='o', label=u'Actual normal data')\n",
    "    ax.scatter(Data_1['Date_Time'].dt.to_pydatetime(), Data_1['Actual'], c=plt.cm.coolwarm(1.), s=200, \n",
    "               edgecolors='y', marker='^', label=u'Actual fault data')\n",
    "    plt.plot(Temp['Date_Time'].dt.to_pydatetime(), Temp['Predicted'], 'c-*', lw = 4, ms = 5, label=u'XGBoost Prediction')\n",
    "    plt.xlabel('Date Time',fontsize=30)\n",
    "    plt.ylabel('Energy Consumption [J]',fontsize=30)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.legend(loc='best',fontsize=30)\n",
    "    plt.savefig('M2-D1-Actual-Labels-Predictions')\n",
    "    \n",
    "    fig = plt.figure(figsize=(25,20))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    Data_0 = Temp.loc[Temp['Pred_Labels'][Temp['Pred_Labels']==0].index]\n",
    "    Data_1 = Temp.loc[Temp['Pred_Labels'][Temp['Pred_Labels']==1].index]\n",
    "    ax.scatter(Data_0['Date_Time'].dt.to_pydatetime(), Data_0['Actual'], c=plt.cm.coolwarm(0.), s=200, \n",
    "               edgecolors='y', marker='o', label=u'Predicted normal data')\n",
    "    ax.scatter(Data_1['Date_Time'].dt.to_pydatetime(), Data_1['Actual'], c=plt.cm.coolwarm(1.), s=200, \n",
    "               edgecolors='y', marker='^', label=u'Predicted fault data')\n",
    "    plt.plot(Temp['Date_Time'].dt.to_pydatetime(), Temp['Predicted'], 'c-*', lw = 4, ms = 5, label=u'XGBoost Prediction')\n",
    "    plt.plot(Temp['Date_Time'].dt.to_pydatetime(), Temp['Threshold'], 'k--', lw = 4, label=u'Dynamic threshold')\n",
    "    plt.xlabel('Date Time',fontsize=30)\n",
    "    plt.ylabel('Energy Consumption [J]',fontsize=30)\n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.legend(loc='best',fontsize=30)\n",
    "    plt.savefig('M2-D1-RF-Dynamic-Threshold-Predicted-Labels')\n",
    "    \n",
    "    t1 = time()\n",
    "    print(\"Time taken: %0.3f\" % (t1-t0))\n",
    "    return scores\n",
    "    #########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = anomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Temp = pd.DataFrame(data=scores,columns=['I','N','ROC_AUC', 'Precision', 'Recall', 'F1_Score'])\n",
    "Data1 = Temp.loc[Temp['N'][Temp['N']==1].index]\n",
    "Data2 = Temp.loc[Temp['N'][Temp['N']==2].index]\n",
    "Data3 = Temp.loc[Temp['N'][Temp['N']==3].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.plot(Data1['I'], Data1['F1_Score'], 'c--s', lw = 6, ms = 10, label=u'$\\mathit{k}$=1')\n",
    "plt.plot(Data2['I'], Data2['F1_Score'], 'r-o', lw = 6, ms = 10, label=u'$\\mathit{k}$=2')\n",
    "plt.plot(Data3['I'], Data3['F1_Score'], 'g--D', lw = 6, ms = 10, label=u'$\\mathit{k}$=3')\n",
    "plt.xlabel('$\\mathit{j}$',fontsize=40)\n",
    "plt.ylabel('F1 score',fontsize=40)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.legend(loc='best',fontsize=40)\n",
    "plt.savefig('M1-D1-Relationship-J-K-F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig1 = plt.figure(figsize=(20,20))\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "plot_acf(y_train,lags=60,ax=ax1)\n",
    "plt.xlabel('Lag', fontsize=40)\n",
    "plt.ylabel('Autocorrelation', fontsize=40)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.figaspect(1.)\n",
    "plt.title(\"\",fontsize=20)\n",
    "plt.savefig('Chiller-ACF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(20,10))\n",
    "ax0 = fig1.add_subplot(1, 1, 1)\n",
    "\n",
    "sns.distplot(y_train, kde=False, color = plt.cm.coolwarm(0.), ax=ax0, norm_hist=True, bins=10)\n",
    "ax0.set_ylabel('Probability',fontsize=40)\n",
    "ax0.set_xlabel('Energy consumption',fontsize=40)\n",
    "\n",
    "plt.savefig('Chiller-Data-Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
